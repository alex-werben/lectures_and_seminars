{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Базовая инфо про токенизаторы"
      ],
      "metadata": {
        "id": "eND5zGrnjYIX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jr8-JjJmb9QM"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "from transformers import (AutoConfig, BertTokenizerFast, AutoTokenizer, AutoModel)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "bert_tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKjwiyu6cAp_",
        "outputId": "eef26722-01c1-45d0-8c18-8b9caf8ff623"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
              "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, key in enumerate(bert_tokenizer.vocab):\n",
        "    print (bert_tokenizer.vocab[key], key)\n",
        "    if idx == 10:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEOOsilGcIDO",
        "outputId": "88ef60de-9327-4e86-d8e4-6b4dfe6574d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20966 harald\n",
            "1713 テ\n",
            "2607 course\n",
            "14898 ##baum\n",
            "22967 ##gues\n",
            "26934 70th\n",
            "26395 anarchy\n",
            "12824 tire\n",
            "1752 二\n",
            "19159 ##cturing\n",
            "24937 saxe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = ['Обычно при использовании API вас прайсят за число токенов'\n",
        "  , 'Usually API pricing is token-based']\n",
        "\n",
        "def print_tokens(texts, tokenizer):\n",
        "  batch = tokenizer.batch_encode_plus(texts\n",
        "                                              , padding='longest' ## добиваем нулями до макс длины в батче\n",
        "                                              , truncation=True\n",
        "                                              , max_length=128\n",
        "                                              , return_tensors=\"pt\")\n",
        "\n",
        "  for idx, text in enumerate(texts):\n",
        "    print(text, '\\n', 'Число токенов ', torch.count_nonzero(batch['input_ids'][idx]), '\\n', batch['input_ids'][idx],  '\\n')"
      ],
      "metadata": {
        "id": "OzR7tUGocUU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_tokens(texts, bert_tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWB8pyl7fu_R",
        "outputId": "0fb70971-e6cf-44a3-beca-070a6b32e91b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Обычно при использовании API вас прайсят за число токенов \n",
            " Число токенов  tensor(45) \n",
            " tensor([  101,  1193, 29740, 29113, 29752, 18947, 14150,  1194, 16856, 10325,\n",
            "         1188, 29747, 29746, 14150, 29436, 23742, 29744, 19259, 28995, 15414,\n",
            "        17928,  1182, 10260, 29747,  1194, 16856, 10260, 10325, 29747, 17432,\n",
            "        22919,  1187, 10260,  1202, 10325, 29747, 29436, 14150,  1197, 14150,\n",
            "        23925, 15290, 18947, 19259,   102]) \n",
            "\n",
            "Usually API pricing is token-based \n",
            " Число токенов  tensor(9) \n",
            " tensor([  101,  2788, 17928, 20874,  2003, 19204,  1011,  2241,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_tokens([texts[1]], bert_tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aQ3Zj6nfyEA",
        "outputId": "cca9c6ab-96b8-463a-b173-bbf426556b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usually API pricing is token-based \n",
            " Число токенов  tensor(9) \n",
            " tensor([  101,  2788, 17928, 20874,  2003, 19204,  1011,  2241,   102]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Итого, 9 токенов против 45 -- стоимость будет отличаться в 5 раз"
      ],
      "metadata": {
        "id": "A4F-KWePf8yp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "попробуем другой токенизатор"
      ],
      "metadata": {
        "id": "KFGJpA5cgzEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizerFast\n",
        "\n",
        "roberta_tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')"
      ],
      "metadata": {
        "id": "cGLZLfYxgx8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_tokens(texts, roberta_tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffblI5lqhD0r",
        "outputId": "a5ab7884-0536-4c44-c960-f156500285a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Обычно при использовании API вас прайсят за число токенов \n",
            " Число токенов  tensor(62) \n",
            " tensor([    0, 25417, 17772, 25417, 15389, 46800, 22063,  6382, 36765, 34251,\n",
            "        18697,  9470, 24269, 35328, 18697, 18537, 36709, 25417,  9470, 41613,\n",
            "         2023, 47015, 25417, 18400, 41613, 14292, 26161, 36765, 35328, 35328,\n",
            "        21013, 18697, 14292, 26161, 36709, 18697,  9470, 24269, 26161, 25417,\n",
            "         9253, 36709, 33162, 35555, 18697, 18400, 26161,  1437, 22063,  6382,\n",
            "        35328, 36709, 40966, 34251,  1437, 35555, 41613,  3070, 25482, 36765,\n",
            "        41613, 14292,     2]) \n",
            "\n",
            "Usually API pricing is token-based \n",
            " Число токенов  tensor(62) \n",
            " tensor([    0, 35808, 21013,  5024,    16, 19233,    12,   805,     2,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "            1,     1,     1]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_tokens([texts[1]], roberta_tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0L1RzwchHy2",
        "outputId": "17a00bc9-586a-4fdd-d56c-bccaa1ea93f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usually API pricing is token-based \n",
            " Число токенов  tensor(8) \n",
            " tensor([    0, 35808, 21013,  5024,    16, 19233,    12,   805,     2]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "уже 62 vs 8 -- здесь разница почти в 8 раз !!!"
      ],
      "metadata": {
        "id": "7QXJh6wthVta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qycL9bZTpCh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Стоит ли связываться со сменой токенизатора?"
      ],
      "metadata": {
        "id": "__PBNPYCjoX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Обучение токенизатора"
      ],
      "metadata": {
        "id": "TLey46BIpDUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если очень хочется -- можно обучить свой токенизатор с помощью\n",
        "\n",
        "```\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "```\n",
        "А затем использовать его через\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3sLXw7fch3jO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "  'возьмем немного текста из анонсов курса'\n",
        "  , \"\"\"\n",
        "  Согласно Gartner https://www.gartner.com/en/newsroom/press-releases/2024-10-21-gartner-identifies-the-top-10-strategic-technology-trends-for-2025 , ИИ-агенты – это топ#1 технологический тренд в 2025 году.\n",
        "  Несмотря на то, что агентские системы известны с прошлого века, и все это время применялись в производстве и логистике, широкое распространение LLM дало новый импульс технологии – и как универсальный оркестратор агента – то есть составление под каждый входящий запрос последовательности действий – планирования пайплайна – состоящего из применения инструментов и/или обращения к другим агентам – и как инструмент, позволяющий легко производить настройку LLM на естественном языке.\n",
        "  Поэтому в курсе на первом занятии мы рассмотрим как выбрать конкретную LLM с учетом имеющихся ограничений, как развернуть ее локально и стоит ли использовать API, как оценить стоимость такого решения и от чего зависит то самое число токенов, по которому ведется прайсинг (а остатся без денег неожиданно легко – как случилось с автором этого поста https://t.me/datarascals/219 ). Упомням как борются с галлюцинациями и повышают точность ответов LLM. Порассуждаем зачем агентам память если уже есть LLM с конктекстом до 10М токенов.  И, чтобы не затягивать – проведем м первое знакомство с тулами и соберем первый агент.\n",
        "\n",
        "  Про второе занятие\n",
        "  Про RAG (Retrieval Augmented Generation) слышал, наверное, каждый, ведь уже 2 года это самый массовый способ применения LLM в проде для бизнеса. Это значит, что точность и надежность такого решения достаточно предсказуемы для того чтобы исключить человека из процесса аппрува выдачи LLM, а связка «локально развернутая LLM + RAG над корпоративными документами» обеспечивает достаточную конфиденциальность чтобы снять тревожность «что наши данные попадут в OpenA» Эта история настолько популярна, что на рынке есть готовые RAG-решения, а опенсорс модели (правда, «маленькие» — до 5B, обзавелись своей RAG-ареной https://huggingface.co/spaces/aizip-dev/SLM-RAG-Arena).\n",
        "  На втором занятии мы соберем свой RAG-пайплайн на своих данных, обсудим,  почему качество такой системы нельзя измерять «на глаз», как правильно собрать схему LLM as a Judge и подготовить дотаяет для нее, что такое guardrails и что делать когда LLM не знает ответ.\n",
        "  Посмотрим на схему агентского RAG и обсудим можно ли применять RAG в качестве памяти агента.\n",
        "  \"\"\"\n",
        " ]\n",
        "\n",
        "with open(\"texts.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in texts:\n",
        "        f.write(line + \"\\n\")"
      ],
      "metadata": {
        "id": "tFtQw1HujUGo"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "files = [\"texts.txt\"] # Путь к файлу с текстами для обучения\n",
        "course_tokenizer = ByteLevelBPETokenizer()\n",
        "course_tokenizer.train(files, vocab_size=30_000, min_frequency=2, special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "])\n",
        "course_tokenizer.save_model(\".\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niy61RQqj-rT",
        "outputId": "54fd8a23-be57-44d5-9b3a-ab0791eff588"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./vocab.json', './merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "course_tokenizer = ByteLevelBPETokenizer(\n",
        "    \"./vocab.json\",\n",
        "    \"./merges.txt\",\n",
        ")\n",
        "\n",
        "course_tokenizer.save(\"./course_tokenizer.json\")"
      ],
      "metadata": {
        "id": "CjHw5gA1lQpr"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### проверим"
      ],
      "metadata": {
        "id": "XMCgKP4Qp8oC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "course_tokenizer_fast = PreTrainedTokenizerFast(tokenizer_file=\"./course_tokenizer.json\")"
      ],
      "metadata": {
        "id": "Oe_A10FzpytN"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "special_tokens = {\n",
        "    \"pad_token\": \"[PAD]\",\n",
        "    \"cls_token\": \"[CLS]\",\n",
        "    \"sep_token\": \"[SEP]\",\n",
        "    \"mask_token\": \"[MASK]\",\n",
        "    \"unk_token\": \"[UNK]\",\n",
        "}\n",
        "for k in special_tokens:\n",
        "    print(f\"{k}: {getattr(course_tokenizer_fast, k)} -> id: {getattr(course_tokenizer_fast, k + '_id')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvVDtiZKpsgG",
        "outputId": "84b8d9bb-395c-4581-d7fe-9a56c3114006"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pad_token: None -> id: None\n",
            "cls_token: None -> id: None\n",
            "sep_token: None -> id: None\n",
            "mask_token: None -> id: None\n",
            "unk_token: None -> id: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "course_tokenizer_fast.add_special_tokens(special_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb-ornjpqLyV",
        "outputId": "d9bed43e-475d-4988-f0ac-a18da3b18637"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for k in special_tokens:\n",
        "    print(f\"{k}: {getattr(course_tokenizer_fast, k)} -> id: {getattr(course_tokenizer_fast, k + '_id')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWfjjdTnqShr",
        "outputId": "6068ab48-fbad-453b-9882-02c3b92804b0"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pad_token: [PAD] -> id: 601\n",
            "cls_token: [CLS] -> id: 602\n",
            "sep_token: [SEP] -> id: 603\n",
            "mask_token: [MASK] -> id: 604\n",
            "unk_token: [UNK] -> id: 605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "save_dir = \"course_tokenizer_fixed\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "course_tokenizer_fast.save_pretrained(save_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGAUnXc_olXt",
        "outputId": "6789fbfd-8cc9-471f-bac3-457d0ba77010"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('course_tokenizer_fixed/tokenizer_config.json',\n",
              " 'course_tokenizer_fixed/special_tokens_map.json',\n",
              " 'course_tokenizer_fixed/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## 2.2. FineTune BERT на наш кастомный токенизатор"
      ],
      "metadata": {
        "id": "53i2F2u2l0u_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "course_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_file=\"course_tokenizer_fixed/tokenizer.json\",\n",
        "    unk_token=\"[UNK]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    cls_token=\"[CLS]\",\n",
        "    sep_token=\"[SEP]\",\n",
        "    mask_token=\"[MASK]\",\n",
        ")"
      ],
      "metadata": {
        "id": "BB_T_7PUsDfQ"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForMaskedLM\n",
        "\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qyzu4oxkq-Od",
        "outputId": "f0b0a234-23f2-4773-c2f0-84ccd6a5a3be"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from transformers.optim import AdamW\n",
        "except ImportError:\n",
        "    try:\n",
        "        from transformers import AdamW\n",
        "    except ImportError:\n",
        "        try:\n",
        "            from transformers.optimization import AdamW\n",
        "        except ImportError:\n",
        "            from torch.optim import AdamW\n",
        "        else:\n",
        "            print('no AdamW')"
      ],
      "metadata": {
        "id": "BtqEwfVMsWb4"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "7N3vF2TwtOzV"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "import torch\n",
        "\n",
        "orig_vocab_size = model.config.vocab_size\n",
        "\n",
        "# # Добавляем новые токены из кастомного токенизатора в модель\n",
        "\n",
        "added_tokens = list(course_tokenizer.get_vocab().keys())[orig_vocab_size:]\n",
        "num_added_tokens = course_tokenizer.add_tokens(added_tokens)\n",
        "\n",
        "if num_added_tokens > 0:\n",
        "    model.resize_token_embeddings(orig_vocab_size + num_added_tokens)\n",
        "\n",
        "\n",
        "\n",
        "# тексты для обучения должны быть из того же корпуса, на котором учили токенизатор\n",
        "texts = None\n",
        "with open(\"texts.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    texts = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "\n",
        "encodings = course_tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "# Data collator для MLM (masked language modeling)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=course_tokenizer, mlm=True, mlm_probability=0.15)\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: val[idx] for key, val in self.encodings.items()}\n",
        "\n",
        "dataset = TextDataset(encodings)\n",
        "loader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=data_collator)\n",
        "\n",
        "\n",
        "\n",
        "# в колабе у нас есть небольшой ресурс GPU, переключить можно в разделе \"среда выполнения\", но потребует перезапуска сеанса (и код заново исполнить придется)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    for batch in loader:\n",
        "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        print(f\"Epoch {epoch} Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aSvSYdUh7fQ",
        "outputId": "01870d21-2c91-4226-e43f-c7dbb3699042"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Loss: 13.866910934448242\n",
            "Epoch 1 Loss: 8.838172912597656\n",
            "Epoch 2 Loss: 7.137530326843262\n",
            "Epoch 3 Loss: 6.8805012702941895\n",
            "Epoch 4 Loss: 6.839742183685303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Выводы  "
      ],
      "metadata": {
        "id": "QsBdzo8_uEZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенизация критически важна в AI-агентах -- как для оценки стоимости использованиях внешних API, так и адаптации под специфику именно вашего кейса.\n",
        "\n",
        "Число токенов, в которое превратится ваш запрос зависит и от токенизатора и от языка запроса (в примере с roberta число токенов отличалось почти в 8 раз на английском и русском языках на одну и ту же фразу). Если в вашем сеттинге используются специфические аббревиатуры / лексика и прочее -- можно добавить токены в токенизатор.\n",
        "\n",
        "В случае простого добавления токенов в токенизаторов модель (LLM) придется фанйтюнить, это не радикально страшно, но потребует контроля изменения метрик качества на разных датасетах."
      ],
      "metadata": {
        "id": "jZpvBNNxuJob"
      }
    }
  ]
}