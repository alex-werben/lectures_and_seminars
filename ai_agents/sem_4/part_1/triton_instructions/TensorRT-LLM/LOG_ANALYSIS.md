# Анализ логов успешного запуска сервера Triton с TensorRT-LLM

Этот документ описывает ключевые моменты в логах, свидетельствующие об успешном запуске сервера Triton Inference Server с бэкендом TensorRT-LLM. Анализ этих логов поможет убедиться, что сервер работает корректно и готов к приему запросов.

## 1. Инициализация и выделение памяти

В самом начале лога можно увидеть сообщения о выделении памяти. Успешная аллокация памяти (pinned и CUDA) является первым шагом.

**На что обратить внимание:**

```
I0720 12:20:55.131099 968 pinned_memory_manager.cc:277] "Pinned memory pool is created at '0x204c00000' with size 268435456"
I0720 12:20:55.131170 968 cuda_memory_manager.cc:107] "CUDA memory pool is created on device 0 with size 67108864"
```

## 2. Инициализация бэкендов

Далее сервер инициализирует необходимые бэкенды. В данном случае это `tensorrtllm` и `python`.

**На что обратить внимание:**
Появление строки `TRITONBACKEND_Initialize: tensorrtllm` и таблицы со списком бэкендов.

```
I0720 12:20:57.378250 968 libtensorrtllm.cc:55] "TRITONBACKEND_Initialize: tensorrtllm"
```

А также итоговая таблица с бэкендами:
```
I0720 12:21:10.213492 968 server.cc:638]
+-------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+
| Backend     | Path                                                            | Config                                                                                                                                                  |
+-------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+
| python      | /opt/tritonserver/backends/python/libtriton_python.so           | {"cmdline":{"auto-complete-config":"false","backend-directory":"/opt/tritonserver/backends","min-compute-capability":"6.000000","shm-region-prefix-name |
|             |                                                                 | ":"prefix0_","default-max-batch-size":"4"}}                                                                                                             |
| tensorrtllm | /opt/tritonserver/backends/tensorrtllm/libtriton_tensorrtllm.so | {"cmdline":{"auto-complete-config":"false","backend-directory":"/opt/tritonserver/backends","min-compute-capability":"6.000000","default-max-batch-size |
|             |                                                                 | ":"4"}}                                                                                                                                                 |
+-------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+
```

## 3. Загрузка моделей

После инициализации бэкендов, Triton начинает загрузку моделей, указанных в `model_repository`.

**На что обратить внимание:**
Сообщения о загрузке (`loading: ...`) и успешном завершении загрузки (`successfully loaded ...`) для каждой модели.

```
I0720 12:20:55.255785 968 model_lifecycle.cc:473] "loading: tensorrt_llm:1"
I0720 12:20:55.259109 968 model_lifecycle.cc:473] "loading: tensorrt_llm_bls:1"
I0720 12:21:10.208708 968 model_lifecycle.cc:849] "successfully loaded 'tensorrt_llm'"
I0720 12:21:10.213380 968 model_lifecycle.cc:849] "successfully loaded 'ensemble'"
```

## 4. Готовность моделей

Ключевой индикатор успеха - таблица со статусом моделей. Все модели должны иметь статус `READY`. Если какая-то модель не загрузилась, в статусе будет указана ошибка.

**На что обратить внимание:**

```
I0720 12:21:10.213538 968 server.cc:681]
+------------------+---------+--------+
| Model            | Version | Status |
+------------------+---------+--------+
| ensemble         | 1       | READY  |
| postprocessing   | 1       | READY  |
| preprocessing    | 1       | READY  |
| tensorrt_llm     | 1       | READY  |
| tensorrt_llm_bls | 1       | READY  |
+------------------+---------+--------+
```

## 5. Запуск сервисов

Финальным этапом является запуск сетевых сервисов для приема запросов (gRPC, HTTP) и метрик.

**На что обратить внимание:**

```
I0720 12:21:10.245090 968 grpc_server.cc:2562] "Started GRPCInferenceService at 0.0.0.0:8001"
I0720 12:21:10.245424 968 http_server.cc:4832] "Started HTTPService at 0.0.0.0:8000"
I0720 12:21:10.328391 968 http_server.cc:358] "Started Metrics Service at 0.0.0.0:8002"
```

Появление этих сообщений в конце лога означает, что сервер полностью запущен, модели загружены, и он готов к работе. 