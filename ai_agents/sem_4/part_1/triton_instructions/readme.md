# Инструкции по запуску Triton Inference Server с различными бэкендами


1.  **[TensorRT-LLM](./TensorRT-LLM/README.md)**: Высокопроизводительный бэкенд от NVIDIA, оптимизированный для графических процессоров NVIDIA.
2.  **[vLLM](./vLLM/README.md)**: Популярный open-source бэкенд, известный своей эффективностью и простотой использования с моделями с Hugging Face.

## Краткое описание бэкендов

### TensorRT-LLM

**TensorRT-LLM** — это решение для максимальной производительности инференса LLM на оборудовании NVIDIA. Оно требует предварительной компиляции модели в специальный формат (engine), что позволяет достичь значительного ускорения за счет различных оптимизаций, таких как:
- Квантизация (FP16, INT8).
- Fused-attention механизмы.
- Оптимизированное управление KV-кэшем.

**Когда использовать:**
- В production-окружениях, где критически важны минимальная задержка (latency) и максимальная пропускная способность (throughput).
- При работе на серверах с GPU от NVIDIA.
- Когда вы готовы потратить время на компиляцию модели для получения максимальной производительности.

Подробное руководство по установке и настройке находится в [TensorRT-LLM/README.md](./TensorRT-LLM/README.md).

### vLLM

**vLLM** — это гибкий и быстрый фреймворк для инференса LLM, который отлично интегрируется с экосистемой Hugging Face. Его ключевая особенность — **PagedAttention**, эффективный алгоритм управления памятью для внимания, который минимизирует потери памяти и позволяет обрабатывать больше запросов параллельно.

**Когда использовать:**
- Для быстрого прототипирования и экспериментов.
- Когда требуется поддержка широкого спектра моделей с Hugging Face "из коробки".
- Когда важна простота настройки и развертывания.

Подробное руководство по работе с локальными и удаленными моделями находится в [vLLM/README.md](./vLLM/README.md).

## Анализ логов

Для каждого бэкенда предоставлены примеры и анализ логов успешного запуска сервера. Эти документы помогут вам диагностировать проблемы и убедиться, что ваш сервер настроен правильно:
- **Анализ логов для TensorRT-LLM**: [TensorRT-LLM/LOG_ANALYSIS.md](./TensorRT-LLM/LOG_ANALYSIS.md)
- **Анализ логов для vLLM**: [vLLM/LOG_ANALYSIS.md](./vLLM/LOG_ANALYSIS.md)

## Как выбрать бэкенд?

| Критерий | TensorRT-LLM | vLLM |
| :--- | :--- | :--- |
| **Производительность** | Максимальная (требует компиляции) | Высокая (с PagedAttention) |
| **Сложность настройки**| Средняя/Высокая | Низкая/Средняя |
| **Поддержка моделей** | Ограниченный список, нужна конвертация | Широкая, интеграция с Hugging Face |
| **Гибкость** | Низкая (после компиляции) | Высокая (легко менять модели) |
| **Основной сценарий**| Production, high-load системы | Исследования, быстрые итерации | 